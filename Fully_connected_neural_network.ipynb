{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IQwlunyE86kE"
   },
   "source": [
    "# Обучение полносвязной нейронной сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T10:27:47.266260Z",
     "start_time": "2020-09-29T10:27:47.262276Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "UyCGfNgA86kF"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VHcumwuF86kL"
   },
   "source": [
    "# Реализация нейронной сети\n",
    "Здесь реализовано обучение полносвязной нейронной сети для распознавания рукописных цифр.\n",
    "Для начала нам понадобится реализовать прямой и обратный проход через слои. Наши слои будут соответствовать следующему интерфейсу (на примере \"тождественного\" слоя):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T10:27:50.721706Z",
     "start_time": "2020-09-29T10:27:50.714677Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "SiqeRVcM86kM"
   },
   "outputs": [],
   "source": [
    "class IdentityLayer:\n",
    "    \"\"\"\n",
    "    A building block. Each layer is capable of performing two things:\n",
    "    \n",
    "    - Process input to get output:           \n",
    "    output = layer.forward(input)\n",
    "    \n",
    "    - Propagate gradients through itself:    \n",
    "    grad_input = layer.backward(input, grad_output)\n",
    "    \n",
    "    Some layers also have learnable parameters.\n",
    "    \n",
    "    Modified code from cs.hse DL course *\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Here you can initialize layer parameters (if any) \n",
    "        and auxiliary stuff. You should enumerate all parameters\n",
    "        in self.params\"\"\"\n",
    "        # An identity layer does nothing\n",
    "        self.params = []\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes input data of shape [batch, input_units], \n",
    "        returns output data [batch, output_units]\n",
    "        \"\"\"\n",
    "        # An identity layer just returns whatever it gets as input.\n",
    "        self.input = input\n",
    "        return input\n",
    "\n",
    "    def backward(self, grad_output): \n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the layer, \n",
    "        with respect to the given input.\n",
    "        \n",
    "        To compute loss gradients w.r.t input, \n",
    "        you need to apply chain rule (backprop):\n",
    "        \n",
    "        d loss / d input  = (d loss / d layer) *  (d layer / d input)\n",
    "        \n",
    "        Luckily, you already receive d loss / d layer as input, \n",
    "        so you only need to multiply it by d layer / d x.\n",
    "        \n",
    "        The method returns:\n",
    "        * gradient w.r.t input (will be passed to \n",
    "          previous layer's backward method)\n",
    "        * flattened gradient w.r.t. parameters (with .ravel() \n",
    "          applied to each gradient). \n",
    "          If there are no params, return []\n",
    "        \"\"\"\n",
    "        # The gradient of an identity layer is precisely grad_output\n",
    "        input_dim = self.input.shape[1]\n",
    "        \n",
    "        d_layer_d_input = np.eye(input_dim)\n",
    "        \n",
    "        return np.dot(grad_output, d_layer_d_input), [] # chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N4IoM_pX86kQ"
   },
   "source": [
    "\n",
    "# Слой нелинейности ReLU\n",
    "Реализуем слой нелинейности $ReLU(x) = max(x, 0)$ (параметров у слоя нет). Метод forward должен вернуть результат поэлементного применения ReLU к входному массиву, метод backward - градиент функции потерь по входу слоя. В нуле будем считать производную равной 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T15:00:30.669741Z",
     "start_time": "2020-09-29T15:00:30.664790Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "y09ZVCsT86kT"
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"\n",
    "    Modified code from cs.hse DL course *\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs\"\"\"\n",
    "        self.params = [] # ReLU has no parameters\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"Apply elementwise ReLU to [batch, num_units] matrix\"\"\"\n",
    "        self.forward_output = np.where(input>0, input, 0)\n",
    "        return self.forward_output \n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"Compute gradient of loss w.r.t. ReLU input\n",
    "        grad_output shape: [batch, num_units]\n",
    "        output 1 shape: [batch, num_units]\n",
    "        output 2: []\n",
    "        \"\"\" \n",
    "        grad = grad_output*(self.forward_output>0)\n",
    "        return grad, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ojTR4GFd86kY"
   },
   "source": [
    "# Полносвязный слой\n",
    "Далее реализуем полносвязный слой без нелинейности. У слоя два параметра: матрица весов и вектор сдвига.\n",
    "\n",
    "Во втором аргументе надо возвращать градиент по всем параметрам в одномерном виде (сначала применяем .ravel() ко всем градиентам, а затем воспользоваться np.r_):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T16:20:41.055530Z",
     "start_time": "2020-09-29T16:20:41.046526Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "YbN5JOc886kZ"
   },
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    \"\"\"\n",
    "    Modified code from cs.hse DL course *\n",
    "    \"\"\"\n",
    "    def __init__(self, input_units, output_units):\n",
    "        \"\"\"\n",
    "        A dense layer is a layer which performs a learned affine transformation:\n",
    "        f(x) = xW + b\n",
    "        \"\"\"\n",
    "        # initialize weights with small random numbers from normal distribution\n",
    "        self.weights = np.random.randn(input_units, output_units)*0.01\n",
    "        self.biases = np.zeros(output_units)\n",
    "        self.params = [self.weights, self.biases]\n",
    "        \n",
    "    def forward(self,input):\n",
    "        \"\"\"\n",
    "        Perform an affine transformation:\n",
    "        f(x) = xW + b\n",
    "        \n",
    "        input shape: [batch, input_units]\n",
    "        output shape: [batch, output units]\n",
    "        \"\"\"\n",
    "        y = input.dot(self.weights) + self.biases\n",
    "        self.input = input\n",
    "        return y\n",
    "        \n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        compute gradients\n",
    "        grad_output shape: [batch, output_units]\n",
    "        output shapes: [batch, input_units], [num_params]\n",
    "        \n",
    "        hint: use function np.r_\n",
    "        np.r_[np.arange(3), np.arange(3)] = [0, 1, 2, 0, 1, 2]\n",
    "        \"\"\"\n",
    "        grad_x = grad_output.dot(self.weights.T)\n",
    "        n = grad_output.shape[0]\n",
    "        grad_w = np.zeros(self.weights.shape)\n",
    "        for i in range(n):\n",
    "            grad_w = grad_w + grad_output[i] * self.input[i].reshape(-1, 1)\n",
    "        grad_b = np.sum(grad_output, axis=0)\n",
    "        return grad_x, np.r_[grad_w.ravel(), grad_b.ravel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T10:28:10.057000Z",
     "start_time": "2020-09-29T10:28:10.043074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "l = Dense(128, 150)\n",
    "assert -0.05 < l.weights.mean() < 0.05 and 1e-3 < l.weights.std() < 1e-1,\\\n",
    "    \"The initial weights must have zero mean and small variance. \"\\\n",
    "    \"If you know what you're doing, remove this assertion.\"\n",
    "assert -0.05 < l.biases.mean() < 0.05, \"Biases must be zero mean. Ignore if you have a reason to do otherwise.\"\n",
    "\n",
    "# To test the outputs, we explicitly set weights with fixed values. DO NOT DO THAT IN ACTUAL NETWORK!\n",
    "l = Dense(3,4)\n",
    "\n",
    "x = np.linspace(-1,1,2*3).reshape([2,3])\n",
    "l.weights = np.linspace(-1,1,3*4).reshape([3,4])\n",
    "l.biases = np.linspace(-1,1,4)\n",
    "\n",
    "assert np.allclose(l.forward(x),np.array([[ 0.07272727,  0.41212121,  0.75151515,  1.09090909],\n",
    "                                          [-0.90909091,  0.08484848,  1.07878788,  2.07272727]]))\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверка градиента\n",
    "Проверим правильность реализации с помощью функции численной проверки градиента. Функция берет на вход callable объект (функцию от одного аргумента-матрицы) и аргумент и вычисляет приближенный градиент функции в этой точке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T10:27:58.741741Z",
     "start_time": "2020-09-29T10:27:58.731723Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval_numerical_gradient(f, x, verbose=False, h=0.00001):\n",
    "    \"\"\"Evaluates gradient df/dx via finite differences:\n",
    "    df/dx ~ (f(x+h) - f(x-h)) / 2h\n",
    "    Adopted from https://github.com/ddtm/dl-course/\n",
    "    \"\"\"\n",
    "    fx = f(x) # evaluate function value at original point\n",
    "    grad = np.zeros_like(x)\n",
    "    # iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "\n",
    "        # evaluate function at x+h\n",
    "        ix = it.multi_index\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h # increment by h\n",
    "        fxph = f(x) # evalute f(x + h)\n",
    "        x[ix] = oldval - h\n",
    "        fxmh = f(x) # evaluate f(x - h)\n",
    "        x[ix] = oldval # restore\n",
    "\n",
    "        # compute the partial derivative with centered formula\n",
    "        grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n",
    "        if verbose:\n",
    "            print (ix, grad[ix])\n",
    "        it.iternext() # step to next dimension\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислите аналитический и численный градиенты по входу слоя ReLU от функции$$ f(y) = \\sum_i y_i, \\quad y = ReLU(x) $$\n",
    "\n",
    "Следующая ячейка после заполнения должна не выдавать ошибку :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T10:28:05.923567Z",
     "start_time": "2020-09-29T10:28:05.914602Z"
    }
   },
   "outputs": [],
   "source": [
    "points = np.linspace(-1,1,10*12).reshape([10,12])\n",
    "l = ReLU()\n",
    "val = l.forward(points)\n",
    "grads = l.backward(np.ones([10,12]))\n",
    "numeric_grads = eval_numerical_gradient(lambda x: l.forward(x).sum(), x=points)\n",
    "assert np.allclose(grads[0], numeric_grads, rtol=1e-3, atol=0),\\\n",
    "    \"gradient returned by your layer does not match the numerically computed gradient\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислите аналитический и численный градиенты по входу полносвязного слоя от функции$$ f(y) = \\sum_i y_i, \\quad y = W x + b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T10:28:55.320281Z",
     "start_time": "2020-09-29T10:28:55.312303Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "0r0vZ1KY86kp"
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 10*12).reshape([10, 12])\n",
    "l = Dense(12, 32,)\n",
    "val = l.forward(x)\n",
    "grads = l.backward(np.ones([10,32]))[0]\n",
    "numeric_grads = eval_numerical_gradient(lambda x: l.forward(x).sum(), x=points)\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0dyhDg0D86kt"
   },
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ArL0HLGH86ku"
   },
   "source": [
    "Реализация softmax-слоя и функции потерь\n",
    "Для решения задачи многоклассовой классификации обычно используют softmax в качестве нелинейности на последнем слое, чтобы получить вероятности классов для каждого объекта:$$\\hat y = softmax(x)  = \\bigl \\{\\frac {exp(x_i)}{\\sum_j exp(x_j)} \\bigr \\}_{i=1}^K, \\quad K - \\text{число классов}$$В этом случае удобно оптимизировать логарифм правдоподобия:$$L(y, \\hat y) = -\\sum_{i=1}^K y_i \\log \\hat y_i \\rightarrow \\min,$$где $y_i=1$, если объект принадлежит $i$-му классу, и 0 иначе. Записанная в таком виде, эта функция потерь совпадает с выражением для кросс-энтропии. Очевидно, что ее также можно переписать через индексацию, если через $y_i$ обозначить класс данного объекта:$$L(y, \\hat y) = - \\log \\hat y_{y_i} \\rightarrow \\min$$В таком виде ее удобно реализовывать.\n",
    "\n",
    "Реализуем слой Softmax (без параметров). Метод forward должен вычислять логарифм от softmax, а метод backward - пропускать градиенты. В общем случае в промежуточных вычислениях backward получится трехмерный тензор, однако для нашей конкретной функции потерь все вычисления можно реализовать в матричном виде. Поэтому мы будем предполагать, что аргумент grad_output - это матрица, у которой в каждой строке только одно ненулевое значение (не обязательно единица)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:16:52.197500Z",
     "start_time": "2020-09-29T20:16:52.193514Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "g27f9NI186ku"
   },
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp, softmax\n",
    "# use this function instead of np.log(np.sum(np.exp(...))) !\n",
    "# because it is more stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:17:06.765472Z",
     "start_time": "2020-09-29T20:17:06.758457Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "xSV3XD0N86ky"
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Applies softmax to each row and then applies component-wise log\n",
    "        Input shape: [batch, num_units]\n",
    "        Output shape: [batch, num_units]\n",
    "        \"\"\"\n",
    "        self.n = input.shape[0]\n",
    "        self.m = input.shape[1]\n",
    "        self.str_max_input = input.max(axis=1).reshape(self.n,1)\n",
    "        y = input - self.str_max_input - logsumexp(input - self.str_max_input, axis=1).reshape(self.n, 1)\n",
    "        self.input = input\n",
    "        return y\n",
    "        \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Propagartes gradients.\n",
    "        Assumes that each row of grad_output contains only 1 \n",
    "        non-zero element\n",
    "        Input shape: [batch, num_units]\n",
    "        Output shape: [batch, num_units]\n",
    "        Do not forget to return [] as second value (grad w.r.t. params)\n",
    "        \"\"\"\n",
    "        grad_x = np.zeros((self.n, self.m))\n",
    "        i, j = np.where(grad_output==-1)\n",
    "        denom = np.exp(self.input - self.str_max_input).sum(axis=1)\n",
    "        grad_x = - np.exp(self.input - self.str_max_input)/denom.reshape(self.n,1)\n",
    "        grad_x[i,j] = 1 + grad_x[i,j]\n",
    "        return -grad_x, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hyPPzkCI86k7"
   },
   "source": [
    "Реализуем функцию потерь и градиенты функции потерь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T13:52:04.923447Z",
     "start_time": "2020-09-29T13:52:04.917463Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "x0lUQZIN86k8"
   },
   "outputs": [],
   "source": [
    "def crossentropy(activations, target):\n",
    "    \"\"\"\n",
    "    returns negative log-likelihood of target under model represented by\n",
    "    activations (log probabilities of classes)\n",
    "    each arg has shape [batch, num_classes]\n",
    "    output shape: 1 (scalar)\n",
    "    \"\"\"\n",
    "    loss = -(target*activations).sum()\n",
    "    return loss \n",
    "    \n",
    "    \n",
    "\n",
    "def grad_crossentropy(activations, target):\n",
    "    \"\"\"\n",
    "    returns gradient of negative log-likelihood w.r.t. activations\n",
    "    each arg has shape [batch, num_classes]\n",
    "    output shape: [batch, num-classes]\n",
    "    \n",
    "    hint: this is just one-hot encoding of target vector\n",
    "          multiplied by -1\n",
    "    \"\"\"\n",
    "    return - target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2wLJgw1A86lB"
   },
   "source": [
    "# Загрузка данных\n",
    "Мы реализаовали все архитектурные составляющие нашей нейронной сети. Осталось загрузить данные и обучить модель. Мы будем работать с датасетом digits, каждый объект в котором - это 8x8 изображение рукописной цифры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T10:29:25.357478Z",
     "start_time": "2020-09-29T10:29:25.350520Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "OVJvVXXQ86lB"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T10:29:25.706165Z",
     "start_time": "2020-09-29T10:29:25.701213Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "bz8eGZ3r86lF"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T10:29:26.152010Z",
     "start_time": "2020-09-29T10:29:26.070193Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "3QnpueaU86lL"
   },
   "outputs": [],
   "source": [
    "X, y = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T10:29:26.895754Z",
     "start_time": "2020-09-29T10:29:26.891789Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 8, 9, 8])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f596OpXk86lO"
   },
   "outputs": [],
   "source": [
    "### make one hot target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T12:12:42.651100Z",
     "start_time": "2020-09-29T12:12:42.645118Z"
    }
   },
   "outputs": [],
   "source": [
    "one_hot_y = np.zeros((y.size, y.max()+1))\n",
    "one_hot_y[np.arange(y.size),y] = 1\n",
    "one_hot_y = one_hot_y.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1797, 64), (1797, 10))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, one_hot_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WXyzDFMH86lW"
   },
   "source": [
    "Разделим данные на обучение и контроль:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T10:29:32.978297Z",
     "start_time": "2020-09-29T10:29:32.974314Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "4vYMOr0H86lW"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T12:12:46.907735Z",
     "start_time": "2020-09-29T12:12:46.900705Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "k0nYXVTG86la"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, target, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T12:12:47.620283Z",
     "start_time": "2020-09-29T12:12:47.614300Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "w-8Ee-d186ld"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1347, 64), (450, 64))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T12:12:48.236234Z",
     "start_time": "2020-09-29T12:12:48.229275Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1347, 10), (450, 10))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kmzKDpyE86lg"
   },
   "source": [
    "# Сборка и обучение нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mPqlZfj_86lg"
   },
   "source": [
    "В нашей реализации нейросеть - это список слоев. Например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T16:20:49.636645Z",
     "start_time": "2020-09-29T16:20:49.631663Z"
    },
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "Sf2sNDJN86lh"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "network = []\n",
    "hidden_layers_size = 32\n",
    "network.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(hidden_layers_size, 10))\n",
    "network.append(Softmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ltomU8p886lk"
   },
   "source": [
    "Для проверки, хорошо ли сеть обучилась, нам понадобится вычислять точность (accuracy) на данной выборке. Для этого реализуем функцию, которая делает предсказания на каждом объекте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T13:52:11.852919Z",
     "start_time": "2020-09-29T13:52:11.848912Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "SV6uAjR286ll"
   },
   "outputs": [],
   "source": [
    "def predict(network, X):\n",
    "    \"\"\"\n",
    "    returns predictions for each object in X\n",
    "    network: list of layers\n",
    "    X: raw data\n",
    "    X shape: [batch, features_num]\n",
    "    output: array of classes, each from 0 to 9\n",
    "    output shape: [batch]\n",
    "    \"\"\"\n",
    "    for layer in network:\n",
    "        val = layer.forward(X)\n",
    "        X = val\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h2nGs4Rq86lo"
   },
   "source": [
    "Мы будем обучать параметры нейросети с помощью готовой функции оптимизации из модуля scipy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T10:29:43.455934Z",
     "start_time": "2020-09-29T10:29:43.451910Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "94lKImJt86lo"
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m-ffcJkz86lw"
   },
   "source": [
    "\n",
    "Эта функция имеет стандартный интерфейс: нужно передать callable объект, который вычисляет значение и градиент целевой функции, а также точку старта оптимизации - начальное приближение (одномерный numpy-массив). Поэтому нам понадобятся функции для сбора и задания всех весов нашей нейросети (именно для них мы всегда записывали параметры слоя в список layer.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T10:29:46.249321Z",
     "start_time": "2020-09-29T10:29:46.241381Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "8tm-JrYb86lw"
   },
   "outputs": [],
   "source": [
    "def get_weights(network):\n",
    "    weights = []\n",
    "    for layer in network:\n",
    "        for param in layer.params:\n",
    "            weights += param.ravel().tolist()\n",
    "    return np.array(weights)\n",
    "\n",
    "def set_weights(weights, network):\n",
    "    i = 0\n",
    "    for layer in network:\n",
    "        for param in layer.params:\n",
    "            l = param.size\n",
    "            param[:] = weights[i:i+l].\\\n",
    "                             reshape(param.shape)\n",
    "            i += l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2uNWIOxe86lz"
   },
   "source": [
    "Реализуем ту самую функцию, которую будем передавать в minimize. Эта функция должна брать на вход текущую точку (вектор всех параметров), а также список дополнительных параметров (мы будем передавать через них нашу сеть и обучающие данные) и возвращать значение критерия качества (кросс-энтропия) и его градиент по параметрам модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T16:20:54.078475Z",
     "start_time": "2020-09-29T16:20:54.071464Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5iq4nkWu86l0"
   },
   "outputs": [],
   "source": [
    "def compute_loss_grad(weights, args):\n",
    "    \"\"\"\n",
    "    takes current weights and computes cross-entropy and gradients\n",
    "    weights shape: [num_parameters]\n",
    "    output 1: loss (scalar)\n",
    "    output 2: gradint w.r.t. weights, shape: [num_parameters]\n",
    "    \n",
    "    hint: firstly perform forward pass through the whole network\n",
    "    then compute loss and its gradients\n",
    "    then perform backward pass, transmitting first baskward output\n",
    "    to the previos layer and saving second backward output in a list\n",
    "    finally flatten all the gradients in this list\n",
    "    (in the order from the first to the last layer)\n",
    "    \n",
    "    Do not forget to set weights of the network!\n",
    "    \"\"\"\n",
    "    network, X, y = args\n",
    "    set_weights(weights, network)\n",
    "    val = predict(network, X)\n",
    "    loss = crossentropy(val, y)\n",
    "    grad_loss = grad_crossentropy(val, y)\n",
    "    grad_param = []\n",
    "    for layer in list(reversed(network)):\n",
    "        cur_grad = layer.backward(grad_loss)\n",
    "        if cur_grad[1] != []:\n",
    "            grad_param.append(cur_grad[1])\n",
    "        grad_loss = cur_grad[0]\n",
    "    grad_param = reversed(grad_param)\n",
    "    v = np.array([])\n",
    "    for item in grad_param:\n",
    "        v = np.r_[v, item]\n",
    "    return loss, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GHJmt1pD86l3"
   },
   "source": [
    "Теперь мы готовы обучать нашу нейросеть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T15:56:11.959485Z",
     "start_time": "2020-09-29T15:56:11.955495Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Lb_t8iaz86l3"
   },
   "outputs": [],
   "source": [
    "weights = get_weights(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T16:21:04.464492Z",
     "start_time": "2020-09-29T16:21:00.522026Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "XTYD1r2P86l7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-ba41149ed0c0>:25: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if cur_grad[1] != []:\n"
     ]
    }
   ],
   "source": [
    "res = minimize(compute_loss_grad, weights,  # fun and start point\n",
    "               args=[network, X_train, y_train], # args passed to fun\n",
    "               method=\"L-BFGS-B\", # optimization method\n",
    "               jac=True) # says that gradient are computed in fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T16:21:09.007808Z",
     "start_time": "2020-09-29T16:21:09.002853Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "9PP9pI3G86l_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fun', 'jac', 'nfev', 'njev', 'nit', 'status', 'message', 'x', 'success', 'hess_inv'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T16:21:09.523101Z",
     "start_time": "2020-09-29T16:21:09.518078Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "b_4bFVQp86mC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"nit\"] # number of iterations (should be >> 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T16:21:10.228606Z",
     "start_time": "2020-09-29T16:21:10.224632Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "3IL7UoCx86mE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"success\"] # should be True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T16:21:11.351300Z",
     "start_time": "2020-09-29T16:21:11.344355Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "djipcHJ786mH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.94619868e-03, -1.88090805e-03,  8.02784727e-03, ...,\n",
       "       -1.09178339e+00, -2.66342375e+00, -5.76350405e-01])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"x\"] # leraned weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T16:21:14.179645Z",
     "start_time": "2020-09-29T16:21:14.173658Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3466,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['x'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sxF7dSlv86mK"
   },
   "source": [
    "Выведем качество на обучении (X_train, y_train) и на контроле (X_test, y_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T17:09:32.831767Z",
     "start_time": "2020-09-29T17:09:32.826776Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(network, X, y):\n",
    "    index = np.argmax(predict(network, X), axis=1) \n",
    "    pred = np.zeros(y.shape)\n",
    "    size = y.shape[0]\n",
    "    pred[np.arange(size),index] = 1\n",
    "    pred = pred.astype('int') \n",
    "    count = 0\n",
    "    for i in range(size):\n",
    "        count += pred[i].dot(y[i])\n",
    "    return count/size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество на (X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T17:00:59.376268Z",
     "start_time": "2020-09-29T17:00:59.362342Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(network, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество на (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T17:09:46.761998Z",
     "start_time": "2020-09-29T17:09:46.753057Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(network, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lzuOHEPY86mO"
   },
   "source": [
    "У minimize есть также аргумент callback - в нее можно передать функцию, которая будет вызываться после каждой итерации оптимизации. Такую функцию удобно оформить в виде метода класса, который будет сохранять качество на обучении контроле после каждой итерации. Реализуем этот метод в классе Callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T17:40:31.350686Z",
     "start_time": "2020-09-29T17:40:31.340745Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "BWx3rls786mP"
   },
   "outputs": [],
   "source": [
    "class Callback:\n",
    "    def __init__(self, network, X_train, y_train, X_test, y_test, print=False):\n",
    "        self.network = network\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.print = print\n",
    "        self.train_acc = []\n",
    "        self.test_acc = []\n",
    "        \n",
    "    def call(self, weights):\n",
    "        \"\"\"\n",
    "        computes quality on train and test set with given weights\n",
    "        and saves to self.train_acc and self.test_acc\n",
    "        if self.print is True, also prints these 2 values\n",
    "        \"\"\"\n",
    "        set_weights(weights, self.network)\n",
    "        train = accuracy(self.network, self.X_train, self.y_train)\n",
    "        test = accuracy(self.network, self.X_test, self.y_test)\n",
    "        self.train_acc.append(train)\n",
    "        self.test_acc.append(test)\n",
    "        if self.print:\n",
    "            print('Train:', train)\n",
    "            print('Test:', test)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T17:40:37.930164Z",
     "start_time": "2020-09-29T17:40:33.485041Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Omd3wRNE86mS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-ba41149ed0c0>:25: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if cur_grad[1] != []:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.10616184112843356\n",
      "Test: 0.08444444444444445\n",
      "Train: 0.24870081662954716\n",
      "Test: 0.21777777777777776\n",
      "Train: 0.2524127691165553\n",
      "Test: 0.2222222222222222\n",
      "Train: 0.23459539717891612\n",
      "Test: 0.24444444444444444\n",
      "Train: 0.2212323682256867\n",
      "Test: 0.23555555555555555\n",
      "Train: 0.25686711210096513\n",
      "Test: 0.28\n",
      "Train: 0.40014847809948034\n",
      "Test: 0.4\n",
      "Train: 0.38530066815144765\n",
      "Test: 0.35555555555555557\n",
      "Train: 0.4610244988864143\n",
      "Test: 0.44666666666666666\n",
      "Train: 0.5330363771343727\n",
      "Test: 0.5511111111111111\n",
      "Train: 0.5293244246473645\n",
      "Test: 0.5422222222222223\n",
      "Train: 0.5649591685226429\n",
      "Test: 0.5733333333333334\n",
      "Train: 0.5419450631031922\n",
      "Test: 0.5311111111111111\n",
      "Train: 0.6117297698589458\n",
      "Test: 0.6044444444444445\n",
      "Train: 0.6310319227913883\n",
      "Test: 0.6177777777777778\n",
      "Train: 0.7030438010393467\n",
      "Test: 0.68\n",
      "Train: 0.6533036377134372\n",
      "Test: 0.6555555555555556\n",
      "Train: 0.7260579064587973\n",
      "Test: 0.6755555555555556\n",
      "Train: 0.7498144023756496\n",
      "Test: 0.7044444444444444\n",
      "Train: 0.7557535263548627\n",
      "Test: 0.7288888888888889\n",
      "Train: 0.7824795842613215\n",
      "Test: 0.7733333333333333\n",
      "Train: 0.7861915367483296\n",
      "Test: 0.76\n",
      "Train: 0.7899034892353378\n",
      "Test: 0.7733333333333333\n",
      "Train: 0.7906458797327395\n",
      "Test: 0.7733333333333333\n",
      "Train: 0.791388270230141\n",
      "Test: 0.7844444444444445\n",
      "Train: 0.8099480326651819\n",
      "Test: 0.8022222222222222\n",
      "Train: 0.8262806236080178\n",
      "Test: 0.82\n",
      "Train: 0.8337045285820341\n",
      "Test: 0.8488888888888889\n",
      "Train: 0.8389012620638456\n",
      "Test: 0.8577777777777778\n",
      "Train: 0.8559762435040832\n",
      "Test: 0.8533333333333334\n",
      "Train: 0.8596881959910914\n",
      "Test: 0.8577777777777778\n",
      "Train: 0.8596881959910914\n",
      "Test: 0.8577777777777778\n",
      "Train: 0.8604305864884929\n",
      "Test: 0.8533333333333334\n",
      "Train: 0.8671121009651076\n",
      "Test: 0.8622222222222222\n",
      "Train: 0.8693392724573126\n",
      "Test: 0.8755555555555555\n",
      "Train: 0.8849294729027468\n",
      "Test: 0.8888888888888888\n",
      "Train: 0.8938381588715665\n",
      "Test: 0.8888888888888888\n",
      "Train: 0.8990348923533779\n",
      "Test: 0.8977777777777778\n",
      "Train: 0.8990348923533779\n",
      "Test: 0.8977777777777778\n",
      "Train: 0.9034892353377877\n",
      "Test: 0.9\n",
      "Train: 0.9005196733481812\n",
      "Test: 0.8977777777777778\n",
      "Train: 0.902746844840386\n",
      "Test: 0.8977777777777778\n",
      "Train: 0.9131403118040089\n",
      "Test: 0.9022222222222223\n",
      "Train: 0.9153674832962138\n",
      "Test: 0.9088888888888889\n",
      "Train: 0.9198218262806236\n",
      "Test: 0.9\n",
      "Train: 0.9220489977728286\n",
      "Test: 0.9\n",
      "Train: 0.9183370452858204\n",
      "Test: 0.9088888888888889\n",
      "Train: 0.9220489977728286\n",
      "Test: 0.9022222222222223\n",
      "Train: 0.9279881217520416\n",
      "Test: 0.9022222222222223\n",
      "Train: 0.9361544172234595\n",
      "Test: 0.9111111111111111\n",
      "Train: 0.9302152932442465\n",
      "Test: 0.9155555555555556\n",
      "Train: 0.947290274684484\n",
      "Test: 0.9244444444444444\n",
      "Train: 0.947290274684484\n",
      "Test: 0.92\n",
      "Train: 0.9487750556792873\n",
      "Test: 0.92\n",
      "Train: 0.9480326651818857\n",
      "Test: 0.9177777777777778\n",
      "Train: 0.9487750556792873\n",
      "Test: 0.9288888888888889\n",
      "Train: 0.9517446176688938\n",
      "Test: 0.9333333333333333\n",
      "Train: 0.9517446176688938\n",
      "Test: 0.9311111111111111\n",
      "Train: 0.9547141796585004\n",
      "Test: 0.9333333333333333\n",
      "Train: 0.9547141796585004\n",
      "Test: 0.9422222222222222\n",
      "Train: 0.9569413511507052\n",
      "Test: 0.9444444444444444\n",
      "Train: 0.9599109131403119\n",
      "Test: 0.9444444444444444\n",
      "Train: 0.9591685226429102\n",
      "Test: 0.9488888888888889\n",
      "Train: 0.9569413511507052\n",
      "Test: 0.9555555555555556\n",
      "Train: 0.9606533036377134\n",
      "Test: 0.9466666666666667\n",
      "Train: 0.9643652561247216\n",
      "Test: 0.9466666666666667\n",
      "Train: 0.9673348181143281\n",
      "Test: 0.9422222222222222\n",
      "Train: 0.9665924276169265\n",
      "Test: 0.9511111111111111\n",
      "Train: 0.9673348181143281\n",
      "Test: 0.9533333333333334\n",
      "Train: 0.9695619896065331\n",
      "Test: 0.9511111111111111\n",
      "Train: 0.9695619896065331\n",
      "Test: 0.9488888888888889\n",
      "Train: 0.9710467706013363\n",
      "Test: 0.9488888888888889\n",
      "Train: 0.9695619896065331\n",
      "Test: 0.9488888888888889\n",
      "Train: 0.9732739420935412\n",
      "Test: 0.9488888888888889\n",
      "Train: 0.9784706755753526\n",
      "Test: 0.9444444444444444\n",
      "Train: 0.9792130660727543\n",
      "Test: 0.9511111111111111\n",
      "Train: 0.9821826280623608\n",
      "Test: 0.9577777777777777\n",
      "Train: 0.985894580549369\n",
      "Test: 0.9533333333333334\n",
      "Train: 0.9851521900519673\n",
      "Test: 0.9577777777777777\n",
      "Train: 0.9821826280623608\n",
      "Test: 0.9555555555555556\n",
      "Train: 0.9829250185597624\n",
      "Test: 0.9533333333333334\n",
      "Train: 0.9851521900519673\n",
      "Test: 0.9511111111111111\n",
      "Train: 0.9851521900519673\n",
      "Test: 0.9511111111111111\n",
      "Train: 0.9851521900519673\n",
      "Test: 0.9533333333333334\n",
      "Train: 0.9866369710467706\n",
      "Test: 0.9533333333333334\n",
      "Train: 0.9873793615441723\n",
      "Test: 0.9511111111111111\n",
      "Train: 0.985894580549369\n",
      "Test: 0.9555555555555556\n",
      "Train: 0.9881217520415738\n",
      "Test: 0.9533333333333334\n",
      "Train: 0.9866369710467706\n",
      "Test: 0.9533333333333334\n",
      "Train: 0.9903489235337788\n",
      "Test: 0.9533333333333334\n",
      "Train: 0.9910913140311804\n",
      "Test: 0.96\n",
      "Train: 0.991833704528582\n",
      "Test: 0.9622222222222222\n",
      "Train: 0.9925760950259837\n",
      "Test: 0.9577777777777777\n",
      "Train: 0.9925760950259837\n",
      "Test: 0.9555555555555556\n",
      "Train: 0.994060876020787\n",
      "Test: 0.9555555555555556\n",
      "Train: 0.9970304380103935\n",
      "Test: 0.9555555555555556\n",
      "Train: 0.9970304380103935\n",
      "Test: 0.9466666666666667\n",
      "Train: 0.9985152190051967\n",
      "Test: 0.9466666666666667\n",
      "Train: 0.9985152190051967\n",
      "Test: 0.9466666666666667\n",
      "Train: 0.9985152190051967\n",
      "Test: 0.9511111111111111\n",
      "Train: 0.9985152190051967\n",
      "Test: 0.9488888888888889\n",
      "Train: 0.9985152190051967\n",
      "Test: 0.9488888888888889\n",
      "Train: 0.9985152190051967\n",
      "Test: 0.9466666666666667\n",
      "Train: 0.9992576095025983\n",
      "Test: 0.9511111111111111\n",
      "Train: 0.9992576095025983\n",
      "Test: 0.9466666666666667\n",
      "Train: 0.9977728285077951\n",
      "Test: 0.9466666666666667\n",
      "Train: 0.9992576095025983\n",
      "Test: 0.9444444444444444\n",
      "Train: 0.9992576095025983\n",
      "Test: 0.9444444444444444\n",
      "Train: 0.9992576095025983\n",
      "Test: 0.9422222222222222\n",
      "Train: 0.9992576095025983\n",
      "Test: 0.9422222222222222\n",
      "Train: 0.9992576095025983\n",
      "Test: 0.9444444444444444\n",
      "Train: 0.9992576095025983\n",
      "Test: 0.9444444444444444\n",
      "Train: 1.0\n",
      "Test: 0.9466666666666667\n",
      "Train: 0.9992576095025983\n",
      "Test: 0.9466666666666667\n",
      "Train: 1.0\n",
      "Test: 0.9488888888888889\n",
      "Train: 1.0\n",
      "Test: 0.9511111111111111\n",
      "Train: 1.0\n",
      "Test: 0.9511111111111111\n",
      "Train: 1.0\n",
      "Test: 0.9466666666666667\n",
      "Train: 1.0\n",
      "Test: 0.9466666666666667\n",
      "Train: 1.0\n",
      "Test: 0.9422222222222222\n",
      "Train: 1.0\n",
      "Test: 0.9377777777777778\n",
      "Train: 1.0\n",
      "Test: 0.9422222222222222\n",
      "Train: 1.0\n",
      "Test: 0.9422222222222222\n",
      "Train: 1.0\n",
      "Test: 0.94\n",
      "Train: 1.0\n",
      "Test: 0.9377777777777778\n",
      "Train: 1.0\n",
      "Test: 0.9377777777777778\n",
      "Train: 1.0\n",
      "Test: 0.94\n",
      "Train: 1.0\n",
      "Test: 0.9355555555555556\n",
      "Train: 1.0\n",
      "Test: 0.9355555555555556\n",
      "Train: 1.0\n",
      "Test: 0.9355555555555556\n",
      "Train: 1.0\n",
      "Test: 0.9377777777777778\n",
      "Train: 1.0\n",
      "Test: 0.9377777777777778\n",
      "Train: 1.0\n",
      "Test: 0.9377777777777778\n",
      "Train: 1.0\n",
      "Test: 0.9355555555555556\n",
      "Train: 1.0\n",
      "Test: 0.9355555555555556\n",
      "Train: 1.0\n",
      "Test: 0.9333333333333333\n",
      "Train: 1.0\n",
      "Test: 0.9333333333333333\n",
      "Train: 1.0\n",
      "Test: 0.9311111111111111\n",
      "Train: 1.0\n",
      "Test: 0.9311111111111111\n",
      "Train: 1.0\n",
      "Test: 0.9311111111111111\n",
      "Train: 1.0\n",
      "Test: 0.9288888888888889\n",
      "Train: 1.0\n",
      "Test: 0.9311111111111111\n",
      "Train: 1.0\n",
      "Test: 0.9311111111111111\n",
      "Train: 1.0\n",
      "Test: 0.9311111111111111\n",
      "Train: 1.0\n",
      "Test: 0.9333333333333333\n",
      "Train: 1.0\n",
      "Test: 0.9333333333333333\n",
      "Train: 1.0\n",
      "Test: 0.9355555555555556\n",
      "Train: 1.0\n",
      "Test: 0.9355555555555556\n",
      "Train: 1.0\n",
      "Test: 0.9355555555555556\n",
      "Train: 1.0\n",
      "Test: 0.9355555555555556\n",
      "Train: 1.0\n",
      "Test: 0.9355555555555556\n",
      "Train: 1.0\n",
      "Test: 0.9377777777777778\n",
      "Train: 1.0\n",
      "Test: 0.9377777777777778\n",
      "Train: 1.0\n",
      "Test: 0.9377777777777778\n",
      "Train: 1.0\n",
      "Test: 0.9377777777777778\n",
      "Train: 1.0\n",
      "Test: 0.9377777777777778\n",
      "Train: 1.0\n",
      "Test: 0.9377777777777778\n",
      "Train: 1.0\n",
      "Test: 0.9377777777777778\n",
      "Train: 1.0\n",
      "Test: 0.9377777777777778\n",
      "Train: 1.0\n",
      "Test: 0.9377777777777778\n",
      "Train: 1.0\n",
      "Test: 0.9377777777777778\n",
      "Train: 1.0\n",
      "Test: 0.94\n",
      "Train: 1.0\n",
      "Test: 0.94\n",
      "Train: 1.0\n",
      "Test: 0.9422222222222222\n",
      "Train: 1.0\n",
      "Test: 0.9422222222222222\n",
      "Train: 1.0\n",
      "Test: 0.9422222222222222\n",
      "Train: 1.0\n",
      "Test: 0.9444444444444444\n",
      "Train: 1.0\n",
      "Test: 0.9444444444444444\n",
      "Train: 1.0\n",
      "Test: 0.9444444444444444\n",
      "Train: 1.0\n",
      "Test: 0.9444444444444444\n",
      "Train: 1.0\n",
      "Test: 0.9444444444444444\n",
      "Train: 1.0\n",
      "Test: 0.9444444444444444\n",
      "Train: 1.0\n",
      "Test: 0.9422222222222222\n",
      "Train: 1.0\n",
      "Test: 0.9422222222222222\n",
      "Train: 1.0\n",
      "Test: 0.9422222222222222\n",
      "Train: 1.0\n",
      "Test: 0.9422222222222222\n",
      "Train: 1.0\n",
      "Test: 0.9422222222222222\n",
      "Train: 1.0\n",
      "Test: 0.94\n",
      "Train: 1.0\n",
      "Test: 0.94\n",
      "Train: 1.0\n",
      "Test: 0.94\n",
      "Train: 1.0\n",
      "Test: 0.94\n",
      "Train: 1.0\n",
      "Test: 0.94\n",
      "Train: 1.0\n",
      "Test: 0.94\n",
      "Train: 1.0\n",
      "Test: 0.94\n",
      "Train: 1.0\n",
      "Test: 0.94\n"
     ]
    }
   ],
   "source": [
    "cb = Callback(network, X_train, y_train, X_test, y_test, print=True)\n",
    "res = minimize(compute_loss_grad, weights,  \n",
    "               args=[network, X_train, y_train], \n",
    "               method=\"L-BFGS-B\",\n",
    "               jac=True,\n",
    "               callback=cb.call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "40YCwfWI86mV"
   },
   "source": [
    "Изобразим на графике кривую качества на обучени и контроле по итерациям:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:29:04.019828Z",
     "start_time": "2020-09-29T20:29:03.866236Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "qSXra9Ub86mV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2449247ad60>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cb.train_acc, label=\"train acc\")\n",
    "plt.plot(cb.test_acc, label=\"test acc\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "keGygn6X86mY"
   },
   "source": [
    "## Эксперименты с числом слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-HqHg3bD86mc"
   },
   "source": [
    "Ясно, что из-за случайного начального приближения с каждым запуском обучения мы будем получать различное качество. Попробуем обучать нашу нейросеть с разным числом слоев несколько раз.\n",
    "\n",
    "Заполним матрицы accs_train и accs_test. В позиции [i, j] должна стоять величина точности сети с $i+1$ полносвязными слоями при $j$-м запуске (все запуски идентичны)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T19:47:18.444291Z",
     "start_time": "2020-09-29T19:47:18.440373Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "tyx-Jf8R86mg"
   },
   "outputs": [],
   "source": [
    "accs_test = np.zeros((5, 5))\n",
    "accs_train = np.zeros((5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:03:36.178098Z",
     "start_time": "2020-09-29T20:01:11.835015Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "uC_ygVJV86mj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-ba41149ed0c0>:25: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if cur_grad[1] != []:\n"
     ]
    }
   ],
   "source": [
    "for j in range(5):\n",
    "    networks = []\n",
    "    network1 = []\n",
    "    hidden_layers_size = 32\n",
    "    network1.append(Dense(X_train.shape[1], 10))\n",
    "    network1.append(Softmax())\n",
    "    networks.append(network1)\n",
    "\n",
    "    network2 = []\n",
    "    network2.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "    network2.append(ReLU())\n",
    "    network2.append(Dense(hidden_layers_size, 10))\n",
    "    network2.append(Softmax())\n",
    "    networks.append(network2)\n",
    "\n",
    "    network3 = []\n",
    "    network3.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "    network3.append(ReLU())\n",
    "    network3.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "    network3.append(ReLU())\n",
    "    network3.append(Dense(hidden_layers_size, 10))\n",
    "    network3.append(Softmax())\n",
    "    networks.append(network3)\n",
    "\n",
    "    network4 = []\n",
    "    network4.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "    network4.append(ReLU())\n",
    "    network4.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "    network4.append(ReLU())\n",
    "    network4.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "    network4.append(ReLU())\n",
    "    network4.append(Dense(hidden_layers_size, 10))\n",
    "    network4.append(Softmax())\n",
    "    networks.append(network4)\n",
    "\n",
    "    network5 = []\n",
    "    network5.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "    network5.append(ReLU())\n",
    "    network5.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "    network5.append(ReLU())\n",
    "    network5.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "    network5.append(ReLU())\n",
    "    network5.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "    network5.append(ReLU())\n",
    "    network5.append(Dense(hidden_layers_size, 10))\n",
    "    network5.append(Softmax())\n",
    "    networks.append(network5)\n",
    "    for i in range(5):\n",
    "        cb = Callback(networks[i], X_train, y_train, X_test, y_test, print=False)\n",
    "        weights = get_weights(networks[i])\n",
    "        res = minimize(compute_loss_grad, weights,\n",
    "                       args=[networks[i], X_train, y_train],\n",
    "                       method=\"L-BFGS-B\",\n",
    "                       jac=True,\n",
    "                       callback=cb.call)\n",
    "        accs_train[i,j] = cb.train_acc[-1]\n",
    "        accs_test[i,j] = cb.test_acc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:03:46.822998Z",
     "start_time": "2020-09-29T20:03:46.815021Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94444444, 0.94666667, 0.94      , 0.94222222, 0.94666667],\n",
       "       [0.94444444, 0.95777778, 0.93777778, 0.96      , 0.94888889],\n",
       "       [0.95111111, 0.94666667, 0.95555556, 0.93333333, 0.96      ],\n",
       "       [0.95111111, 0.93555556, 0.94888889, 0.95333333, 0.94666667],\n",
       "       [0.92      , 0.93333333, 0.94888889, 0.08444444, 0.91777778]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ik2G2TbG86mp"
   },
   "source": [
    "Построим боксплоты полученного качества (горизонтальная линия в каждом столбце - среднее, прямоугольник показывает разброс)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:03:51.571848Z",
     "start_time": "2020-09-29T20:03:51.407289Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "UyUp3bX_86mp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Test quality in 5 runs')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(accs_test.T, showfliers=False)\n",
    "plt.xlabel(\"Num layers\")\n",
    "plt.ylabel(\"Test accuracy\")\n",
    "plt.title(\"Test quality in 5 runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AAztSPvw86mt"
   },
   "source": [
    "Как изменяются качество на обучении и контроле и устойчивость процесса обучения при увеличении числа слоев?\n",
    "* Лучшее среднее значение качества получено при 3х слоях, с 3 до 5 качество убывает. В данном эксперименте при 5 слоях получился наибольший разброс, при одном слое разброс наименьший, значит, более устойчивый процесс обучения.\n",
    "\n",
    "Несколько фрагментов кода в задании написаны на основе материалов курса по глубинному обучению на ФКН НИУ ВШЭ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "HW1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
